# Model Configuration for Mistral-7B-Instruct-v0.3
# Configuration for the entertainment-specialized model

# Base Model Information
base_model:
  name: "mistralai/Mistral-7B-Instruct-v0.3"
  architecture: "mistral"
  parameters: "7B"
  context_length: 32768
  vocab_size: 32000

# Quantization Settings
quantization:
  method: null  # Disabled for L40S fp16 inference
  compute_dtype: "float16"
  use_nested_quant: false

# Generation Parameters
generation:
  default:
    max_new_tokens: 1024
    temperature: 0.7
    top_k: 50
    repetition_penalty: 1.2
    do_sample: true
    pad_token_id: 2
    eos_token_id: 2
  creative_writing:
    temperature: 0.8
    top_k: 40
    repetition_penalty: 1.05
  factual_qa:
    temperature: 0.3
    top_k: 30
    repetition_penalty: 1.2
  recommendation:
    temperature: 0.4
    top_k: 30
    repetition_penalty: 1.1

# Performance Settings
performance:
  use_cache: true
  low_cpu_mem_usage: true
  torch_dtype: "float16"
  device_map: "cuda"
  max_memory: "32GB"

# Tokenizer Settings
tokenizer:
  model_max_length: 32768
  padding_side: "left"
  truncation_side: "left"
  add_eos_token: true
  add_bos_token: true
  special_tokens:
    pad_token: "</s>"
    eos_token: "</s>"
    bos_token: "<s>"
    unk_token: "<unk>"

# Model Loading Settings
loading:
  max_position_embeddings: 32768
  rope_scaling: null
  use_litgpt: false
  fabric_accelerator: "cuda"
  fabric_devices: 1
  torch_load_fallback: true
  strict_loading: false
  retry_attempts: 3
  timeout_seconds: 300